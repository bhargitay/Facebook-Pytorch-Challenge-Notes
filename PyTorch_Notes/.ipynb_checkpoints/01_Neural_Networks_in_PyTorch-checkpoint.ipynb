{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks in PyTorch\n",
    "\n",
    "[PyTorch](http://pytorch.org/) is a framework for building and training neural networks. \n",
    "\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "- Deep Learning is based on artificial Neural Networks (NNs) \n",
    "- the NNs are built from individual neurons (also called units) \n",
    "- the goal of the NN is to learn to recognize patterns in your data\n",
    "- once the NN has been trained on samples of your data, it can make predictions by detecting similar patterns in future data\n",
    "\n",
    "### Layers of Neural Networks \n",
    "NNs have certain special architecture with layers: \n",
    "\n",
    "![Layers of a Neural Network](./imgs/nn_layers.png)\n",
    "\n",
    "\n",
    "**input layer**\n",
    "- the 1st layer\n",
    "- contains the inputs ($x_1, x_2...x_n$)\n",
    "\n",
    "**hidden layer**\n",
    "- a set of linear models created with the input layer\n",
    "\n",
    "**output layer**\n",
    "- final layer\n",
    "- where the linear models get combined to obtain a nonlinear model \n",
    "\n",
    "### What is happening inside the node?\n",
    "\n",
    "If we would zoom in to one of the hidden or output nodes, we would see the following: \n",
    "\n",
    "<img src=\"nn_layers_zoom.png\" width=300px>\n",
    "\n",
    "1. Each unit has some weights and a bias \n",
    "    - they are updated during the network training depending on the error\n",
    "2. These weighted inputs are summed together (= a linear combination) \n",
    "3. Then passed through an activation function to get the unit's output\n",
    " \n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activation function:</b> \n",
    "<li> puts a nonlinear transformation to the linear combination, which generates the output </li>\n",
    "<li> the output of the activation function is used as a input in the next layer </li>\n",
    "<li> examples: sigmoid, softmax, tanh, ReLu function </li>\n",
    "</div>\n",
    "\n",
    "Mathematically this looks like: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "You can think of input and weights as vectors.<br> \n",
    "With vectors this is the dot/inner product of two vectors:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Tensors\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Tensors:</b> \n",
    "<li> are a generalization of vectors and matrices </li>\n",
    "<li> the fundamental data structure for neural networks  </li>\n",
    "<li> PyTorch (and other deep learning frameworkas as well) is built around them </li>\n",
    "<li> behaves like numpy arrays </li>\n",
    "</div>\n",
    "\n",
    "**1D tensor**:\n",
    "- a vector (is an instance of a tensor)\n",
    "- we just have a single 1D array of values\n",
    "<br>\n",
    "\n",
    "**2D tensor**: \n",
    "- matrix\n",
    "- we have values going in 2 directions --> from left to right and from top to bottom\n",
    " * so that we have individual rows + columns\n",
    " * we can do operations accross the columns\n",
    " \n",
    "<br>\n",
    "**3D tensor**:\n",
    "- an array with 3 indices (e.g. RGB color images) \n",
    " * for every pixel there's some value for all red, green and blue channels --> for every individual pixel in a 2D image, we have 3 values\n",
    "<br>\n",
    "\n",
    "You can actually have 4D, 5D, 6D ... etc. tensors, but we normally work with 1D, 2D and 3D tensors. <br> \n",
    "\n",
    "![Tensors](./imgs/tensor_examples.svg)\n",
    "\n",
    "- NNs computations are just a bunch of linear algebra operations on tensors <br>\n",
    "\n",
    "- PyTorch tensors \n",
    "    - can be added, multiplied, subtracted, etc, just like Numpy arrays.\n",
    "    - we'll use them  pretty much the same way we'd use Numpy arrays\n",
    "    - they come with some nice benefits though such as GPU acceleration \n",
    "\n",
    "## Numpy vs Pytorch \n",
    "\n",
    "PyTorch \n",
    "- takes these tensors and makes it simple to move them to GPUs for the faster processing when training neural networks\n",
    "- provides modules that \n",
    "    - automatically calculates gradients (for backpropagation!) and \n",
    "    - another specifically for building neural networks\n",
    "\n",
    "All together, PyTorch ends up being more coherent with Python and the Numpy/Scipy stack compared to TensorFlow and other frameworks. <br>\n",
    "\n",
    "Numpy arrays can easily be replaced with tensorflow’s tensor, but the reverse is not true.\n",
    "\n",
    "\n",
    "## EXERCISE 1: Calculate the output of  a single neuron\n",
    "<br> \n",
    "\n",
    "### STEPS\n",
    "1. Set each unit random weights and a bias \n",
    "2. Calculate the output of the network <br> \n",
    "    2.1. Sum the weighted inputs together (= a linear combination)  <br> \n",
    "    2.2. Then pass through an activation function to get the unit's output <br> \n",
    "\n",
    "![Simple Neuron](./imgs/simple_neuron.png)\n",
    "\n",
    "**Parts of this neural network:** <br>\n",
    "$x_1, x_2, ... x_n$: input features (row vector) - features of the input data for your network <br>\n",
    "$w_1, w_2, ... w_n$: weights (row vector) <br>\n",
    "$h$: activation function <br>\n",
    "$y$: output (prediction)\n",
    "\n",
    "\n",
    "### 1. Generating random data for features + weights, and set bias \n",
    "\n",
    "#### STEPS \n",
    "1. We generate some random data for features (matrix(1,5) - a vector with 5 elements) \n",
    "2. We generate some random data for weights (matrix(1,5) - a vector with 5 elements) \n",
    "3. Set bias (matrix(1,1))\n",
    "\n",
    "> #### PYTORCH HELP \n",
    "> ##### Random Sampling\n",
    "\n",
    "> `torch.manual_seed(seed)`\n",
    "-  Sets the seed for generating random numbers.\n",
    "-  We need to set this because we want to get the same each number each time we run this notebook. \n",
    ">\n",
    ">`torch.rand(sizes)`\n",
    " - returns a tensor filled with random numbers from a uniform distribution \n",
    " - size: \n",
    "    * defining the shape of the tensor \n",
    "    * a tuple/list of the size that you want\n",
    ">\n",
    "> `torch.randn_like(input)`\n",
    " - returns a tensor \n",
    "    * with the same size as input\n",
    "    * that is filled with random numbers from a uniform distribution\n",
    " - input: a tensor which shape we would like to copy \n",
    " - is equivalent to torch.rand(input.size()) \n",
    "\n",
    ">[Random Sampling (Documentation)](https://pytorch.org/docs/stable/torch.html#random-sampling)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x240f676f2d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We set the random seed so we can reproduce the same results each time we use this given seed. \n",
    "torch.manual_seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Random features \n",
    "\n",
    "This case we want the features to be a matrix: \n",
    "- a 2D tensor of 1 row and 5 columns (= a row vector that has 5 elements) \n",
    "- containing random normal distributed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = torch.randn((1, 5))\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Random Weights  \n",
    "we create another 2D tensor \n",
    "- with the same shape as `features`\n",
    "- again containing values from a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.randn_like(features)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Set bias\n",
    "we create a single value for the bias \n",
    "- also from a normal distibution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3177]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and a true bias term\n",
    "bias = torch.randn((1, 1))\n",
    "bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculating the output of the neuron\n",
    "\n",
    "Use the generated data to calculate the output of this simple single layer network.\n",
    "\n",
    "#### STEPS \n",
    "1. Define the activation function (sigmoid) \n",
    "2. Calculate the output of the neuron\n",
    "\n",
    "#### 2.1. Define the activation function \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Sigmoid function:</b> \n",
    "   <li> squeezes the input values between 0 and 1 </li>\n",
    "   <li> really useful for providing a probability </li>\n",
    "    <li>  if you want your NN to output a probability, then sigmoid is what you want to use </li>\n",
    "</div>\n",
    "\n",
    "![Sigmoid](./imgs/sigmoid.png)\n",
    "\n",
    "\n",
    ">####  PYTORCH HELP\n",
    "> `torch.exp(x)`\n",
    "- Returns a new tensor with the exponential of the elements of the input tensor input\n",
    ">\n",
    "> [torch.exp() (Documentation)](https://pytorch.org/docs/stable/torch.html?highlight=exp#torch.exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">####  PYTORCH HELP\n",
    "> `torch.sum(input)` + `tensor.sum(input)`\n",
    "- doing the same\n",
    "- sums up all the elements in the given input tensor \n",
    "  \n",
    "> [torch.sum() (Documentation)](https://pytorch.org/docs/stable/torch.html?highlight=torch%20sum#torch.sum)  \n",
    "\n",
    "\n",
    "### 2.1. Solution 1 - torch.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1595]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = activation(torch.sum(features*weights) + bias)\n",
    "y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Solution 2 - tensor.sum()\n",
    "**Steps**:\n",
    "1. we're doing elementwise multiplication \n",
    "2. we sum up all the values in the tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1595]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = activation((features * weights).sum() + bias)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Solution 3 - Matrix multiplications \n",
    "\n",
    "\n",
    "- You can do the multiplication and sum in the same operation using a matrix multiplication. \n",
    "- In general, we'll want to use matrix multiplications since\n",
    "    * they are more efficient and \n",
    "    * these linear algebra oprations have been accelerated using modern libraries and high-performance computing on GPUs\n",
    "\n",
    "> #### PYTORCH HELP \n",
    "\n",
    "> ##### Matrix multiplication\n",
    "> - there are 2 ways to do matrix multiplication in Pytorch\n",
    "> - both functions' result is the matrix product of two tensors\n",
    "\n",
    ">`torch.matmul(tensor1, tensor2)`\n",
    "- supports broadcasting \n",
    "    * its behavior depends on the dimensionality of the tensors\n",
    "\n",
    "><div class=\"alert alert-block alert-success\">\n",
    "<b>Broadcasting:</b> <br>\n",
    "if a PyTorch operation supports broadcast, then its Tensor arguments can be automatically expanded to be of equal sizes <br>\n",
    "(without making copies of the data)<br>\n",
    "</div>\n",
    "        \n",
    "\n",
    ">`torch.mm(tensor1, tensor2,)`\n",
    "- does not broadcast \n",
    "    - more simple and more strict about the tensors that you pass in\n",
    "    - if we got something wrong, it's going throw an error instead of just doing it and continuing the calculations \n",
    "        * e.g. If we try to do it with `features` and `weights` as they are, we'll get an error:\n",
    "\n",
    "> ```python\n",
    "> >> torch.mm(features, weights)\n",
    "\n",
    "> ---------------------------------------------------------------------------\n",
    "> RuntimeError                              Traceback (most recent call last)\n",
    "> <ipython-input-13-15d592eb5279> in <module>()\n",
    "> ----> 1 torch.mm(features, weights)\n",
    "\n",
    "> RuntimeError: size mismatch, m1: [1 x 5], m2: [1 x 5] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033\n",
    "> ```\n",
    ">\n",
    "> - we see a RuntimeError: size mismatch error  <br>\n",
    ">    * we'll see this error very often when designing NNs. <br>\n",
    ">    * the problem: our tensors aren't the correct shapes to perform a matrix multiplication <br>\n",
    "\n",
    "> **Most of the error we are going to see when we are buliding networks and lot of the difficulty when it comes to designing the architecture of neural networks is getting the shapes of your tensors to work right together. **\n",
    "-  the large part of debuggin we're actually going to be trying to look at the shape of your tensors as they are going through our network\n",
    "    \n",
    ">`tensor.shape`:\n",
    "- to see the shape of the tensor\n",
    "- it works like this in other deep learning frameworks as well\n",
    "> \n",
    "> [torch.mm (Documentation)](https://pytorch.org/docs/stable/torch.html#torch.mm) <br> \n",
    "> [torch.matmul (Documentation)](https://pytorch.org/docs/stable/torch.html#torch.matmul) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### They have the same shape. What is the problem? <br>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b> REMEBER FROM LINEAR ALGEBRA</b> <br>\n",
    "for matrix multiplications, the number of columns in the first tensor must equal to the number of rows in the second column <br>\n",
    "</div>\n",
    "\n",
    "This means we need to change the shape of `weights` (to 5*1)  to get the matrix multiplication to work.\n",
    "\n",
    "> #### PYTORCH HELP\n",
    "\n",
    "> ##### Reshaping Tensors \n",
    "> There are 3 options: \n",
    ">\n",
    "> `tensor.reshape(a, b)` <br>\n",
    "- sometimes it will return a new tensor with the same data as the tensor with size `(a, b)`, \n",
    "- and sometimes a clone, as in it copies the data to another part of memory.\n",
    "> \n",
    "> `tensor.resize_(a, b)` \n",
    "- returns the same tensor with a different shape. \n",
    "- if the new shape results in \n",
    "    - fewer elements than the original tensor:  some elements will be removed from the tensor (but not from memory)\n",
    "    - more elements than the original tensor: new elements will be uninitialized in memory\n",
    "- this method is performed **in-place**\n",
    "\n",
    "> <div class=\"alert alert-block alert-success\">\n",
    "> <b>In-place operation:</b> \n",
    ">    <li> an operation changes directly the content of a given tensor without making a copy </li>\n",
    ">    <li> in PyTorch they are always postfixed with a _, like .add_() or .scatter_()  </li>\n",
    ">    <li> Python operations like += or *= are also in-place operations  </li> \n",
    "> </div>\n",
    "> \n",
    "> `weights.view(a, b)` \n",
    "> - will return a new tensor with the same data as `weights` with size `(a, b)`\n",
    "> - this is the most recommended to use for reshaping a tensor\n",
    "\n",
    "> [Math Operations(Documentation)](https://pytorch.org/docs/stable/torch.html?highlight=torch%20mm#math-operations) <br>\n",
    "> [Broadcasting (Documentation)](https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics)<br>\n",
    "> [Reshape tensors (Documentation)](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape)<br>\n",
    "> [Resize tensors (Documentation)](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_)<br>\n",
    "> [View tensors (Documentation)](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view)<br>\n",
    "> [In-place operations](https://discuss.pytorch.org/t/what-is-in-place-operation/16244)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8948],\n",
       "        [-0.3556],\n",
       "        [ 1.2324],\n",
       "        [ 0.1382],\n",
       "        [-1.6822]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.view(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.view(5,1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the output of this network using matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1595]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = activation(torch.mm(features, weights.view(5,1)) + bias)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXCERCISE 2: Calculate the output of a  Multilayer Neural Network \n",
    "<br>\n",
    "- We can stack these single neurons up into a multi-layer neural network, and this basically gives our network greater power to capture patterns and correlations in our data. \n",
    "- This way the output of one layer of neurons becomes the input for the next layer. \n",
    "- **With multiple input units and output units, we now need to express the weights as a matrix.**  <br>\n",
    "<br>\n",
    "<br>\n",
    "![Multilayer Neural Network](./imgs/multilayer_diagram_weights.png)\n",
    "\n",
    "**Parts of this neural network:** <br>\n",
    "*the input layer *\n",
    "- $x_1, x_2, ... x_n$: input features (row vector) <br>\n",
    "- $w_1, w_2, ... w_n$: weights (matrix) <br>\n",
    "    * they connect our input to one hidden unit in this middle layers \n",
    "\n",
    "*the hidden layer *\n",
    "- $h$: activation function <br>\n",
    "\n",
    "*the output layer*\n",
    "- $y$: output (prediction)\n",
    "\n",
    "<br> \n",
    "\n",
    "**Get the values of the hidden layer**:\n",
    "- To get the values for the hidden layer,  we do a matrix multiplication between our feature vector ($x_1  to  x_n$) and our weight matrix. \n",
    "- We can express this network mathematically with matrices again and use matrix multiplication to get linear combinations for each unit in one operation. <br> \n",
    "\n",
    "*For example, the hidden layer ($h_1$ and $h_2$ here) can be calculated *\n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "When we multiply our inputs (features) with\n",
    "- the first column of the weight matrix, then we are going to get the output $h_1$. \n",
    "- the second column of the weight matrix, then we are going to get the output $h_2$. \n",
    "\n",
    "\n",
    "The output for this small network is found by treating the hidden layer as inputs for the output unit. The network output is expressed simply\n",
    "\n",
    "$$\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$\n",
    "\n",
    "<br> \n",
    "\n",
    "### STEPS \n",
    "1. Generate some random data for features (matrix(1,3) - row vector)  \n",
    "2. Define the size of each layer in the network (hidden: 2) \n",
    "3. Generate some random data for  <br>\n",
    "    3.1. weights W1, W2 (matrix(1,3)) <br>\n",
    "    3.2. bias terms B1, B2 (matrix(1,1)) <br>\n",
    "4. Calculate the output of the network<br>\n",
    "    4.1. Calculate the result of a single neuron (a hidden layer) <br>\n",
    "    4.2. Stack the neurons up<br>\n",
    "\n",
    "### 1. Generate some random data for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1468,  0.7861,  0.9468]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7)\n",
    "\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn((1, 3))\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the size of each layer in our network\n",
    "\n",
    "It is important so that we would know how many rows and columns are needed in the weight matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = features.shape[1]     # Number of input units, must match number of input features\n",
    "n_hidden = 2                    # Number of hidden units \n",
    "n_output = 1                    # Number of output units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate random data\n",
    "#### 3.1. for weights \n",
    "\n",
    "for the hidden and the output layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights for inputs to hidden layer: tensor([[-1.1143,  1.6908],\n",
      "        [-0.8948, -0.3556],\n",
      "        [ 1.2324,  0.1382]])\n",
      "Weights for hidden layer to output layer: tensor([[-1.6822],\n",
      "        [ 0.3177]])\n"
     ]
    }
   ],
   "source": [
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "print('Weights for inputs to hidden layer:', W1)\n",
    "print('Weights for hidden layer to output layer:', W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. for bias terms \n",
    "\n",
    "for the hidden and the output layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias terms for hidden layer: tensor([[0.1328, 0.1373]])\n",
      "Bias term for output layer: tensor([[0.2405]])\n"
     ]
    }
   ],
   "source": [
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))\n",
    "\n",
    "print('Bias terms for hidden layer:', B1)\n",
    "print('Bias term for output layer:', B2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate the output of the network\n",
    "\n",
    "#### 4.1. calculate the result of a single neuron (a hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6813, 0.4355]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = activation(torch.mm(features, W1) + B1)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Stack the neurons up \n",
    "\n",
    "with using the hidden layer calculated in the previous step as an input for the next layer ofr our NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3171]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = activation(torch.mm(h, W2) + B2)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of hidden units a parameter of the network\n",
    "\n",
    "- are often called a **hyperparameter** to differentiate it from the weights and biases parameters\n",
    "- the more hidden units a network has, and the more layers, the better able it is to learn from data and make accurate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources \n",
    "\n",
    "## Udacity\n",
    "[The Original Notebook](https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-to-pytorch) <br> \n",
    "[Single Layer Neural Networks](https://youtu.be/6Z7WntXays8) <br> \n",
    "[Single layer neural networks solution](https://youtu.be/mNJ8CujTtpo) <br> \n",
    "[Networks Using Matrix Multiplication](https://youtu.be/QLaGMz8Ca3E) <br> \n",
    "[Multilayer Networks Solution](https://youtu.be/iMIo9p5iSbE)\n",
    "[Feedforward](https://youtu.be/hVCuvMGOfyY) <br> \n",
    "\n",
    "\n",
    "## Other \n",
    "[What is a Neural Net?](http://www.cormactech.com/neunet/whatis.html) <br>\n",
    "[Linear Algebra cheatsheet Deep Learning](https://towardsdatascience.com/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c)<br>\n",
    "[25 Must Know Terms & concepts for Beginners in Deep Learning](https://www.analyticsvidhya.com/blog/2017/05/25-must-know-terms-concepts-for-beginners-in-deep-learning/) <br> \n",
    "[Applied Deep Learning - Part 1: Artificial Neural Networks](https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6) <br> \n",
    "[Activation functions and it’s types-Which is better?](https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f) <br> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
